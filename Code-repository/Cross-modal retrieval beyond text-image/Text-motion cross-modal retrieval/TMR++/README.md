<div align="center">

# TMR++
## A Cross-Dataset Study for Text-based 3D Human Motion Retrieval

<a href=""><strong>Léore Bensabath</strong></a>
·
<a href="https://mathis.petrovich.fr"><strong>Mathis Petrovich</strong></a>
·
<a href="https://imagine.enpc.fr/~varolg"><strong>G&#252;l Varol</strong></a>


[![arXiv](https://img.shields.io/badge/arXiv-TMR-A10717.svg?logo=arXiv)](https://arxiv.org/abs/2405.16909)

</div>


## Description
Official PyTorch implementation of the paper:
<div align="center">

[**A Cross-Dataset Study for Text-based 3D Human Motion Retrieval**](https://arxiv.org/abs/2405.16909).

</div>

This repo is based on the implementation of 
[**TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis**](https://github.com/Mathux/TMR/tree/master).

Please visit our [**webpage**](https://imagine.enpc.fr/~leore.bensabath/TMR++) for more details.

### Bibtex
If you find this code useful in your research, please cite:

```bibtex
@inproceedings{lbensabath2024,
    title={TMR++: A Cross-Dataset Study for Text-based 3D Human Motion Retrieval},
    author={Bensabath, Léore and Petrovich, Mathis and Varol, G{\"u}l},
    journal={CVPRW HuMoGen},
    year={2024}
}
```
and
```bibtex
@inproceedings{petrovich23tmr,
    title     = {{TMR}: Text-to-Motion Retrieval Using Contrastive {3D} Human Motion Synthesis},
    author    = {Petrovich, Mathis and Black, Michael J. and Varol, G{\"u}l},
    booktitle = {International Conference on Computer Vision ({ICCV})},
    year      = 2023
}
```

You can also put a star :star:, if the code is useful to you.

## Installation :construction_worker:

<details><summary>Create environment</summary>
&emsp;

Create a python virtual environnement:
```bash
python -m venv ~/.venv/TMR
source ~/.venv/TMR/bin/activate
```

Install [PyTorch](https://pytorch.org/get-started/locally/)
```bash
python -m pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

Then install remaining packages:
```
python -m pip install -r requirements.txt
```

which corresponds to the packages: pytorch_lightning, einops, hydra-core, hydra-colorlog, orjson, tqdm, scipy.
The code was tested on Python 3.10.12 and PyTorch 2.0.1.

</details>

<details><summary>Set up the datasets</summary>
&emsp;

Please first set up the datasets as explain in https://github.com/Mathux/TMR/tree/master in the same README section.

In this repo, we provide the augmented versions of dataset humanml3d, kitml and babel. 
For a given dataset ($DATASET), up to 3 new annotation file have been created:
- ``dataset/annotations/$DATASET/annotations_paraphrases.json``: Includes all the paraphrases generated by a llm
- ``dataset/annotations/$DATASET/annotations_actions.json``: For humanml3d and kitml only, includes the action type label generated by a llm
- ``dataset/annotations/$DATASET/annotations_all.json``: Includes a concatenation by key id of all the annotations (original and llm generated)

Copy the data in your repo from [here](https://drive.google.com/drive/u/1/folders/1_SpOgtYCZBPAXoVz00Zhyk6tPRObUIiW)
</details>

<details><summary>Combine datasets</summary>
&emsp;

To create a combination of any of the datasets, run:

```bash
python -m prepare.combine_datasets datasets=$DATASETS test_sets=$TEST_DATSETS split_suffix=$SPLIT_SUFFIX [OPTIONS]
```
Where:
- ``datasets``: The list of datasets to combine
- ``test_sets``: The intended list on which the dataset is going to be tested. When generating the split files, this will filter from the training set the samples from one of the training datasets that overlap with samples from another provided testing dataset. 
Note that you can create different splits for different intended testing sets by leveraging parameter **split_suffix**. The annotations file for the given combination will stay the same regardless of the **test_sets** value.
- ``split_suffix``: The split file suffix for this given combination of test sets. Training and validation split files will be saved under: ``datasets/annotations/splits/train{split_suffix}.txt`` and ``datasets/annotations/splits/val{split_suffix}.txt``

The new dataset will be created inside folder ``datasets/annotations/{dataset1}_{dataset2}(_{dataset3})``

**Example:**
```bash
python -m prepare.combine_datasets datasets=["humanml3d","kitml"] test_sets=["babel"] split_suffix="_wo_hkb"
```
</details>

## [TO BE CONTINUED]

## Training :rocket:

```bash
python train.py [OPTIONS]
```

<details><summary>Details</summary>
&emsp;

By default, it will train TMR on HumanML3D and store the folder in ``outputs/tmr_humanml3d_guoh3dfeats`` which I will call ``RUN_DIR``.
The other options are:

#### Models:
- ``model=tmr``: TMR (by default)
- ``model=temos``: TEMOS

#### Datasets:
- ``data=humanml3d``: HumanML3D (by default)
- ``data=kitml``: KIT-ML
- ``data=babel``: BABEL

</details>

<details><summary>Extracting weights</summary>
&emsp;

After training, run the following command, to extract the weights from the checkpoint:

```bash
python extract.py run_dir=RUN_DIR
```

It will take the last checkpoint by default. This should create the folder ``RUN_DIR/last_weights`` and populate it with the files: ``motion_decoder.pt``, ``motion_encoder.pt`` and ``text_encoder.pt``.
This process makes loading models faster, it does not depends on the file structure anymore, and each module can be loaded independently. This is already done for pretrained models.

</details>

## Pretrained models :dvd:

```bash
bash prepare/download_pretrain_models.sh
```

This will put pretrained models in the ``models`` folder.
Currently, there are:
- TMR trained on HumanML3D with Guo et al. humanml3d features ``models/tmr_humanml3d_guoh3dfeats``
- TMR trained on KIT-ML with Guo et al. humanml3d features ``models/tmr_kitml_guoh3dfeats``

Not that KIT-ML is used with the Guo et al. ``humanml3d`` features (it is not a mistake). The motions come from AMASS and are converted (I am not using the MMM joints from the original KIT-ML).
This makes the two models works in the same motion space.

More models may be available later on.

## Evaluation :bar_chart:

```bash
python retrieval.py run_dir=RUN_DIR
```

It will compute the metrics, show them and save them in this folder ``RUN_DIR/contrastive_metrics/``.


## Usage :computer:

### Encode a motion
Note that the .npy file should corresponds to HumanML3D Guo features.

```bash
python encode_motion.py run_dir=RUN_DIR npy=/path/to/motion.npy
```

### Encode a text

```bash
python encode_text.py run_dir=RUN_DIR text="A person is walking forward."
```

### Compute similarity between text and motion
```bash
python text_motion_sim.py run_dir=RUN_DIR text=TEXT npy=/path/to/motion.npy
```
For example with ``text="a man sets to do a backflips then fails back flip and falls to the ground"`` and ``npy=HumanML3D/HumanML3D/new_joint_vecs/001034.npy`` you should get around 0.96.


## Launch the demo

### Encode the whole motion dataset
```bash
python encode_dataset.py run_dir=RUN_DIR
```


### Text-to-motion retrieval demo
Run this command:

```bash
python app.py
```

and then open your web browser at the address: ``http://localhost:7860``.

## Localization (WIP)

The code will be available a bit later.


### Reimplementation of TEMOS (WIP)

<details><summary>Details and difference</summary>
&emsp;

[TEMOS code](https://github.com/Mathux/TEMOS) was probably a bit too abstract and some users struggle to understand it. As TMR and TEMOS share a similar architecture, I took the opportunity to rewrite TEMOS in this repo [src/model/temos.py](src/model/temos.py) to make it more user friendly. Note that in this repo, the motion representation is different from the original TEMOS paper (see [DATASETS.md](DATASETS.md) for more details). Another difference is that I precompute the token embeddings (from distilbert) beforehand (as I am not finetunning the distilbert for the final model). This makes the training around x2 faster and it is more memory efficient.

The code and the generations are not fully tested yet, I will update the README with pretrained models and more information later.

</details>


## License :books:
This code is distributed under an [MIT LICENSE](LICENSE).

Note that our code depends on other libraries, including PyTorch, PyTorch3D, Hugging Face, Hydra, and uses datasets which each have their own respective licenses that must also be followed.
