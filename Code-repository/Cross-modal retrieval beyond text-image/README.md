# Code Repository for Cross-modal Retrieval beyond Text-Image
We gather representative cross-modal retrievalcodes beyond text-image that are available.

## Text-video cross-modal retrieval
**1. CLIP4Clip**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

**2. Cap4Video**  
- **Source:** http://vision.cs.uiuc.edu/pascal-sentences/  
- **Reference:** C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier, “Collecting image annotations using amazon’s mechanical turk,” in NAACL-HLT, 2010, pp. 139–147.

**3. HGR**  
- **Source:** https://shannon.cs.illinois.edu/DenotationGraph/  
- **Reference:** P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, vol. 2, pp. 67–78, 2014.

**4. Howto100m**  
- **Source:** https://cocodataset.org  
- **Reference:** T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in ECCV, 2014, pp. 740–755.

**5. LJEMC**  
- **Source:** https://shannon.cs.illinois.edu/DenotationGraph/  
- **Reference:** P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, vol. 2, pp. 67–78, 2014.

**6. LSMDC**  
- **Source:** https://shannon.cs.illinois.edu/DenotationGraph/  
- **Reference:** P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, vol. 2, pp. 67–78, 2014.

**7. MEE**  
- **Source:** https://shannon.cs.illinois.edu/DenotationGraph/  
- **Reference:** P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, vol. 2, pp. 67–78, 2014.

**8. MMT**  
- **Source:** https://shannon.cs.illinois.edu/DenotationGraph/  
- **Reference:** P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, vol. 2, pp. 67–78, 2014.

**9. T-MASS**  
- **Source:** https://shannon.cs.illinois.edu/DenotationGraph/  
- **Reference:** P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, vol. 2, pp. 67–78, 2014.

**10. Word2VisualVec**  
- **Source:** https://shannon.cs.illinois.edu/DenotationGraph/  
- **Reference:** P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, vol. 2, pp. 67–78, 2014.

## Text-audio cross-modal retrieval
**1. ATR**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

**2. OML**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

**3. TTMR++**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

## Image-audio cross-modal retrieval
**1. ALAC**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

## Video-audio cross-modal retrieval
**1. AClENet**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

## Image-3D cross-modal retrieval
**1. RONO**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

## Text-3D cross-modal retrieval
**1. RMARN**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

## Text-motion cross-modal retrieval
**1. TMR**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

**2. TMR++**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

## Image-video cross-modal retrieval
**1. APIVR**
- **Source:** http://www.svcl.ucsd.edu/projects/crossmodal/  
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

# Copyright Notice
These datasets in the Data Repository is owned by their original authors. 
Users must acknowledge the original sources when using these datasets.
We collect and provide access to these datasets solely to facilitate research and advance scientific understanding. This repository does not use or distribute these datasets for any commercial purposes.
