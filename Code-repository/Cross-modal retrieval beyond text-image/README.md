# Code Repository for Cross-modal Retrieval beyond Text-Image
We gather representative cross-modal retrieval codes beyond text-image that are available.

## Text-video cross-modal retrieval
**1. CLIP4Clip**  
- **Paper:** https://www.sciencedirect.com/science/article/abs/pii/S0925231222008876
- **Source:** https://github.com/ArrowLuo/CLIP4Clip  
- **Reference:** H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, “Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning,” Neurocomputing, vol. 508, pp. 293–304, 2022.

**2. Cap4Video**  
- **Paper:** https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Cap4Video_What_Can_Auxiliary_Captions_Do_for_Text-Video_Retrieval_CVPR_2023_paper.html
- **Source:** https://github.com/whwu95/Cap4Video
- **Reference:** W. Wu, H. Luo, B. Fang, J. Wang, and W. Ouyang, “Cap4video: What can auxiliary captions do for text-video retrieval?” in IEEE CVPR, 2023, pp. 10704–10713.

**3. HGR**  
- **Paper:** https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Fine-Grained_Video-Text_Retrieval_With_Hierarchical_Graph_Reasoning_CVPR_2020_paper.html
- **Source:** https://github.com/cshizhe/hgr_v2t
- **Reference:** S. Chen, Y. Zhao, Q. Jin, and Q. Wu, “Fine-grained video-text retrieval with hierarchical graph reasoning,” in IEEE CVPR, 2020, pp. 10638–10647.

**4. Howto100m**  
- **Paper:** https://openaccess.thecvf.com/content_ICCV_2019/html/Miech_HowTo100M_Learning_a_Text-Video_Embedding_by_Watching_Hundred_Million_Narrated_ICCV_2019_paper.html
- **Source:** https://github.com/antoine77340/howto100m
- **Reference:** A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, “Howto100m: Learning a text-video embedding by watching hundred million narrated video clips,” in IEEE ICCV, 2019, pp. 2630–2640.

**5. LJEMC**  
- **Paper:** https://dl.acm.org/doi/abs/10.1145/3206025.3206064
- **Source:** https://github.com/niluthpol/multimodal_vtt
- **Reference:** N. C. Mithun, J. Li, F. Metze, and A. K. Roy-Chowdhury, “Learning joint embedding with multimodal cues for cross-modal video-text retrieval,” in ACM ICMR, 2018, pp. 19–27.

**6. LSMDC**  
- **Paper:** https://link.springer.com/article/10.1007/s11263-016-0987-1
- **Source:** https://github.com/yj-yu/lsmdc
- **Reference:** A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, C. Pal, H. Larochelle, A. Courville, and B. Schiele, “Movie description,” IJCV, vol. 123, pp. 94–120, 2017.

**7. MEE**  
- **Paper:** https://arxiv.org/abs/1804.02516
- **Source:** https://github.com/antoine77340/Mixture-of-Embedding-Experts 
- **Reference:** A. Miech, I. Laptev, and J. Sivic, “Learning a text-video embedding from incomplete and heterogeneous data,” arXiv preprint arXiv:1804.02516, 2018.

**8. MMT**  
- **Paper:** https://link.springer.com/chapter/10.1007/978-3-030-58548-8_13
- **Source:** https://github.com/gabeur/mmt
- **Reference:** V. Gabeur, C. Sun, K. Alahari, and C. Schmid, “Multi-modal transformer for video retrieval,” in ECCV, 2020, pp. 214–229.

**9. T-MASS**  
- **Paper:** https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Text_Is_MASS_Modeling_as_Stochastic_Embedding_for_Text-Video_Retrieval_CVPR_2024_paper.html
- **Source:** https://github.com/Jiamian-Wang/T-MASS-text-video-retrieval
- **Reference:** J. Wang, G. Sun, P. Wang, D. Liu, S. Dianat, M. Rabbani, R. Rao, and Z. Tao, “Text is mass: Modeling as stochastic embedding for text-video retrieval,” in IEEE CVPR, 2024, pp. 16551–16560.

**10. Word2VisualVec**  
- **Paper:** https://ieeexplore.ieee.org/abstract/document/8353472
- **Source:** https://github.com/danieljf24/w2vv
- **Reference:** J. Dong, X. Li, and C. G. Snoek, “Predicting visual features from text for image and video caption retrieval,” IEEE TMM, vol. 20, no. 12, pp. 3377–3388, 2018.

## Text-audio cross-modal retrieval
**1. ATR**
- **Paper:** https://ieeexplore.ieee.org/abstract/document/9746786
- **Source:** https://github.com/SiyuLou/AudioTextRetrievalInContext
- **Reference:** S. Lou, X. Xu, M. Wu, and K. Yu, “Audio-text retrieval in context,” in IEEE ICASSP, 2022, pp. 4793–4797.

**2. OML**
- **Paper:** https://arxiv.org/abs/2203.15537
- **Source:** https://github.com/XinhaoMei/audio-text_retrieval
- **Reference:** X. Mei, X. Liu, J. Sun, M. D. Plumbley, and W. Wang, “On metric learning for audio-text cross-modal retrieval,” arXiv preprint arXiv:2203.15537, 2022.

**3. TTMR++**
- **Paper:** https://ieeexplore.ieee.org/abstract/document/10446380
- **Source:** https://github.com/seungheondoh/music-text-representation-pp
- **Reference:** S. Doh, M. Lee, D. Jeong, and J. Nam, “Enriching music descriptions with a finetuned-llm and metadata for text-to-music retrieval,” in IEEE ICASSP, 2024, pp. 826–830.

## Image-audio cross-modal retrieval
**1. ALAC**
- **Paper:** https://ieeexplore.ieee.org/abstract/document/10543059
- **Source:** https://github.com/huangjh98/ALAC
- **Reference:** J. Huang, Y. Chen, S. Xiong, and X. Lu, “Cross-modal remote sensing image-audio retrieval with adaptive learning for aligning correlation,” IEEE TGRS, vol. 62, pp. 1–13, 2024.

## Video-audio cross-modal retrieval
**1. ACIENet**
- **Paper:** https://ieeexplore.ieee.org/abstract/document/10499254
- **Source:** https://github.com/w1018979952/ACIENet
- **Reference:** J. Wang, A. Zheng, Y. Yan, R. He, and J. Tang, “Attribute-guided cross-modal interaction and enhancement for audio-visual matching,” IEEE TIFS, vol. 19, pp. 4986–4998, 2024.

## Image-3D cross-modal retrieval
**1. RONO**
- **Paper:** https://openaccess.thecvf.com/content/CVPR2023/html/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.html
- **Source:** https://github.com/penghu-cs/RONO
- **Reference:** Y. Feng, H. Zhu, D. Peng, X. Peng, and P. Hu, “Rono: robust discriminative learning with noisy labels for 2d-3d cross-modal retrieval,” in IEEE CVPR, 2023, pp. 11610–11619.

## Text-3D cross-modal retrieval
**1. RMARN**
- **Paper:** https://arxiv.org/abs/2408.13712
- **Source:** https://github.com/liwrui/RMARN
- **Reference:** W. Li, W. Han, Y. Chen, Y. Chai, Y. Lu, X. Wang, and X. Fan, “Riemann-based multi-scale attention reasoning network for text-3d retrieval,” arXiv preprint arXiv:2408.13712, 2024.

## Text-motion cross-modal retrieval
**1. TMR**
- **Paper:** https://openaccess.thecvf.com/content/ICCV2023/html/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.html
- **Source:** https://mathis.petrovich.fr/tmr
- **Reference:** M. Petrovich, M. J. Black, and G. Varol, “Tmr: Text-to-motion retrieval using contrastive 3d human motion synthesis,” in IEEE ICCV, 2023, pp. 9488–9497.

**2. TMR++**
- **Paper:** https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/html/Bensabath_A_Cross-Dataset_Study_for_Text-based_3D_Human_Motion_Retrieval_CVPRW_2024_paper.html
- **Source:** https://github.com/leorebensabath/TMRPlusPlus
- **Reference:** L. Bensabath, M. Petrovich, and G. Varol, “A cross-dataset study for text-based 3d human motion retrieval,” in IEEE CVPR, 2024, pp. 1932–1940.

## Image-video cross-modal retrieval
**1. APIVR**
- **Paper:** https://ojs.aaai.org/index.php/AAAI/article/view/6941
- **Source:** https://github.com/xuweitj/Cross-modal-retrieval/tree/master/APIVR-master
- **Reference:** R. Xu, L. Niu, J. Zhang, and L. Zhang, “A proposal-based approach for activity image-to-video retrieval,” in AAAI, vol. 34, no. 07, 2020, pp. 12524–12531.

# Copyright Notice
These codes in the Code Repository are owned by their original authors. 
Users must acknowledge the original sources when using these codes.
We collect and provide access to these codes solely to facilitate research and advance scientific understanding. This repository does not use or distribute these codes for any commercial purposes.
