# Code Repository for Text-Image Cross-modal Retrieval 
We gather representative text-image cross-modal retrieval codes that are available.

## Unsupervised real-value retrieval
**1. CAMP**  
- **Paper:** 
- **Source:** 
- **Reference:**

**2. CCA**  
- **Paper:** 
- **Source:** 
- **Reference:** N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,” in ACM MM, 2010, pp. 251–260.

**3. CDPAE**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Zhan, J. Yu, Z. Yu, R. Zhang, D. Tao, and Q. Tian, “Comprehensive distance-preserving autoencoders for cross-modal retrieval,” in ACM MM, 2018, pp. 1137–1145.

**4. CHAIN-VSE**  
- **Paper:** 
- **Source:** 
- **Reference:** J. Wehrmann and R. C. Barros, “Bidirectional retrieval made simple,” in IEEE CVPR, 2018, pp. 7718–7726.

**5. CMRSC**  
- **Paper:** 
- **Source:** 
- **Reference:**

**6. CRGN**  
- **Paper:** 
- **Source:** 
- **Reference:**

**7. DSRAN**  
- **Paper:** 
- **Source:** 
- **Reference:**

**8. DSVEL**  
- **Paper:** 
- **Source:** 
- **Reference:** M. Engilberge, L. Chevallier, P. Pérez, and M. Cord, “Finding beans in burgers: Deep semantic-visual embedding with localization,” in IEEE CVPR, 2018, pp. 3984–3993.

**9. DCCA**  
- **Paper:** 
- **Source:** 
- **Reference:** G. Andrew, R. Arora, J. A. Bilmes, and K. Livescu, “Deep canonical correlation analysis,” in ICML, vol. 28, 2013, pp. 1247–1255.

**10. IMRAM**  
- **Paper:** 
- **Source:** 
- **Reference:** H. Chen, G. Ding, X. Liu, Z. Lin, J. Liu, and J. Han, “Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval,” in IEEE CVPR, 2020, pp. 12655–12663.

**11. PCME**  
- **Paper:** 
- **Source:** 
- **Reference:** S. Chun, S. J. Oh, R. S. de Rezende, Y. Kalantidis, and D. Larlus, “Probabilistic embeddings for cross-modal retrieval,” in IEEE CVPR, 2021, pp. 8415–8424.

**12. PFAN**  
- **Paper:** 
- **Source:** 
- **Reference:**

**13. PSN**  
- **Paper:** 
- **Source:** 
- **Reference:**

**14. PVSE**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Song and M. Soleymani, “Polysemous visual-semantic embedding for cross-modal retrieval,” in IEEE CVPR, 2019, pp. 1979–1988.

**15. PolyLoss**  
- **Paper:** 
- **Source:** 
- **Reference:**

**16. RRF-Net**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Liu, Y. Guo, E. M. Bakker, and M. S. Lew, “Learning a recurrent residual fusion network for multimodal matching,” in IEEE ICCV, 2017, pp. 4107–4116.

**17. SCAN**  
- **Paper:** 
- **Source:** 
- **Reference:** K. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention for image-text matching,” in ECCV, vol. 11208, 2018, pp. 212–228.

**18. VSEPP**  
- **Paper:** 
- **Source:** 
- **Reference:**

**19. VSRN**  
- **Paper:** 
- **Source:** 
- **Reference:** K. Li, Y. Zhang, K. Li, Y. Li, and Y. Fu, “Visual semantic reasoning for image-text matching,” in IEEE ICCV, 2019, pp. 4653–4661.

**20. X-MRS**  
- **Paper:** 
- **Source:** 
- **Reference:** R. Guerrero, H. X. Pham, and V. Pavlovic, “Cross-modal retrieval and synthesis (X-MRS): closing the modality gap in shared subspace learning,” in ACM MM, 2021, pp. 3192–3201.

## Supervised real-value retrieval
**1. AACR**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Wu, S. Wang, G. Song, and Q. Huang, “Augmented adversarial training for cross-modal retrieval,” IEEE TMM, vol. 23, pp. 559–571, 2021.

**2. ACMR**  
- **Paper:** 
- **Source:** 
- **Reference:** B. Wang, Y. Yang, X. Xu, A. Hanjalic, and H. T. Shen, “Adversarial cross-modal retrieval,” in ACM MM, 2017, pp. 154–162.

**3. CM-GANS**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Peng and J. Qi, “Cm-gans: Cross-modal generative adversarial networks for common representation learning,” ACM TOMM, vol. 15, no. 1, pp. 22:1–22:24, 2019.

**4. DSCMR**  
- **Paper:** 
- **Source:** 
- **Reference:** L. Zhen, P. Hu, X. Wang, and D. Peng, “Deep supervised cross-modal retrieval,” in IEEE CVPR, 2019, pp. 10394–10403.

**5. GCR**  
- **Paper:** 
- **Source:** 
- **Reference:**

**6. GMA**  
- **Paper:** 
- **Source:** 
- **Reference:** A. Sharma, A. Kumar, H. D. III, and D. W. Jacobs, “Generalized multiview analysis: A discriminative latent space,” in IEEE CVPR, 2012, pp. 2160–2167.

**7. JFSE**  
- **Paper:** 
- **Source:** 
- **Reference:**

**8. JFSSL**  
- **Paper:** 
- **Source:** 
- **Reference:** K. Wang, R. He, L. Wang, W. Wang, and T. Tan, “Joint feature selection and subspace learning for cross-modal retrieval,” IEEE TPAMI, vol. 38, no. 10, pp. 2010–2023, 2016.

**9. JGRHML**  
- **Paper:** 
- **Source:** 
- **Reference:**

**10. MNiL**  
- **Paper:** 
- **Source:** 
- **Reference:**

**11. MVMLCCA**  
- **Paper:** 
- **Source:** 
- **Reference:**

**12. deep-SM**  
- **Paper:** 
- **Source:** 
- **Reference:**

**13. ml-CCA**  
- **Paper:** 
- **Source:** 
- **Reference:** V. Ranjan, N. Rasiwasia, and C. V. Jawahar, “Multi-label cross-modal retrieval,” in IEEE ICCV, 2015, pp. 4094–4102.

## Unsupervised hashing retrieval
**1. DGCPN**  
- **Paper:** 
- **Source:** 
- **Reference:** J. Yu, H. Zhou, Y. Zhan, and D. Tao, “Deep graph-neighbor coherence preserving network for unsupervised cross-modal hashing,” in AAAI, 2021, pp. 4626–4634.

**2. DJSRH**  
- **Paper:** 
- **Source:** 
- **Reference:** S. Su, Z. Zhong, and C. Zhang, “Deep joint-semantics reconstructing hashing for large-scale unsupervised cross-modal retrieval,” in IEEE ICCV, 2019, pp. 3027–3035.

**3. DSAH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**4. FSH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**5. JDSH**  
- **Paper:** 
- **Source:** 
- **Reference:** S. Liu, S. Qian, Y. Guan, J. Zhan, and L. Ying, “Joint-modal distribution-based similarity hashing for large-scale unsupervised deep cross-modal retrieval,” in ACM SIGIR, 2020, pp. 1379–1388.

**6. RFDH**  
- **Paper:** 
- **Source:** 
- **Reference:** D. Wang, Q. Wang, and X. Gao, “Robust and flexible discrete hashing for cross-modal similarity search,” IEEE TCSVT, vol. 28, no. 10, pp. 2703–2715, 2018.

**7. UGACH**  
- **Paper:** 
- **Source:** 
- **Reference:** J. Zhang, Y. Peng, and M. Yuan, “Unsupervised generative adversarial cross-modal hashing,” in AAAI, 2018, pp. 539–546.

## Supervised hashing retrieval
**1. AGAH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**2. ASCSH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**3. BATCH**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Wang, X. Luo, L. Nie, J. Song, W. Zhang, and X. Xu, “BATCH: A scalable asymmetric discrete cross-modal hashing,” IEEE TKDE, vol. 33, no. 11, pp. 3507–3519, 2021.

**4. Bi-CMR**  
- **Paper:** 
- **Source:** 
- **Reference:**

**5. CDQ**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Cao, M. Long, J. Wang, and S. Liu, “Collective deep quantization for efficient cross-modal retrieval,” in AAAI, 2017, pp. 3974–3980.

**6. CPAH**  
- **Paper:** 
- **Source:** 
- **Reference:** D. Xie, C. Deng, C. Li, X. Liu, and D. Tao, “Multi-task consistency-preserving adversarial hashing for cross-modal retrieval,” IEEE TIP, vol. 29, pp. 3626–3637, 2020.

**7. DADH**  
- **Paper:** 
- **Source:** 
- **Reference:** C. Bai, C. Zeng, Q. Ma, J. Zhang, and S. Chen, “Deep adversarial discrete hashing for cross-modal retrieval,” in ACM ICMR, 2020, pp. 525–531.

**8. DCHMT**  
- **Paper:** 
- **Source:** 
- **Reference:** J. Tu, X. Liu, Z. Lin, R. Hong, and M. Wang, “Differentiable cross-modal hashing via multimodal transformers,” in ACM MM, 2022, pp. 453–461.

**9. DCHUC**  
- **Paper:** 
- **Source:** 
- **Reference:** R. Tu, X. Mao, B. Ma, Y. Hu, T. Yan, W. Wei, and H. Huang, “Deep cross-modal hashing with hashing functions and unified hash codes jointly learning,” IEEE TKDE, vol. 34, no. 2, pp. 560–572, 2022.

**10. DCMH**  
- **Paper:** 
- **Source:** 
- **Reference:** Q. Jiang and W. Li, “Deep cross-modal hashing,” in IEEE CVPR, 2017, pp. 3270–3278.

**11. DLFH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**12. GCH**  
- **Paper:** 
- **Source:** 
- **Reference:**  R. Xu, C. Li, J. Yan, C. Deng, and X. Liu, “Graph convolutional network hashing for cross-modal retrieval,” in IJCAI, 2019, pp. 982–988.

**13. LGCNH**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Chen, S. Wang, J. Lu, Z. Chen, Z. Zhang, and Z. Huang, “Local graph convolutional networks for cross-modal hashing,” in ACM MM, 2021, pp. 1921–1928.

**14. MDBE**  
- **Paper:** 
- **Source:** 
- **Reference:** D. Wang, X. Gao, X. Wang, L. He, and B. Yuan, “Multimodal discriminative binary embedding for large-scale cross-modal retrieval,” IEEE TIP, vol. 25, no. 10, pp. 4540–4554, 2016.

**15. RaHH**  
- **Paper:** 
- **Source:** 
- **Reference:** M. Ou, P. Cui, F. Wang, J. Wang, W. Zhu, and S. Yang, “Comparing apples to oranges: a scalable solution with heterogeneous hashing,” in ACM KDD, 2013, pp. 230–238.

**16. ROPH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**17. SCAHN**  
- **Paper:** 
- **Source:** 
- **Reference:**

**18. SCRATCH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**19. SRSH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**20. SSAH**  
- **Paper:** 
- **Source:** 
- **Reference:** C. Li, C. Deng, N. Li, W. Liu, X. Gao, and D. Tao, “Self-supervised adversarial hashing networks for cross-modal retrieval,” in IEEE CVPR, 2018, pp. 4242–4251.

## Cross-modal retrieval under special scenarios
**1. CMOLRS**  
- **Paper:** 
- **Source:** 
- **Reference:**

**2. DECL**  
- **Paper:** 
- **Source:** 
- **Reference:** Y. Qin, D. Peng, X. Peng, X. Wang, and P. Hu, “Deep evidential learning with noisy correspondence for cross-modal retrieval,” in ACM MM, 2022, pp. 4948–4956.

**3. ECMH**  
- **Paper:** 
- **Source:** 
- **Reference:** T. Chen, L. Zhang, S. Zhang, Z. Li, and B. Huang, “Extensible cross-modal hashing,” in IJCAI, 2019, pp. 2109–2115.

**4. FGCrossNet**  
- **Paper:** 
- **Source:** 
- **Reference:** X. He, Y. Peng, and L. Xie, “A new benchmark and approach for fine-grained cross-media retrieval,” in ACM MM, 2019, pp. 1740–1748.

**5. FOMH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**6. GSPH**  
- **Paper:** 
- **Source:** 
- **Reference:** D. Mandal, K. N. Chaudhury, and S. Biswas, “Generalized semantic preserving hashing for n-label cross-modal retrieval,” in IEEE CVPR, 2017, pp. 2633–2641.

**7. HichNet**  
- **Paper:** 
- **Source:** 
- **Reference:** C. Sun, X. Song, F. Feng, W. X. Zhao, H. Zhang, and L. Nie, “Supervised hierarchical cross-modal hashing,” in ACM SIGIR, 2019, pp. 725–734.

**8. LEMON**  
- **Paper:** 
- **Source:** 
- **Reference:**

**9. MRL**  
- **Paper:** 
- **Source:** 
- **Reference:** P. Hu, X. Peng, H. Zhu, L. Zhen, and J. Lin, “Learning cross-modal retrieval with noisy labels,” in IEEE CVPR, 2021, pp. 5403–5413.

**10. SCH-GAN**  
- **Paper:** 
- **Source:** 
- **Reference:**

**11. SHDCH**  
- **Paper:** 
- **Source:** 
- **Reference:**

**12. TFNH**  
- **Paper:** 
- **Source:** 
- **Reference:** Z. Hu, X. Liu, X. Wang, Y. Cheung, N. Wang, and Y. Chen, “Triplet fusion network hashing for unpaired cross-modal retrieval,” in ACM ICMR, 2019, pp. 141–149.

# Copyright Notice
These codes in the Code Repository are owned by their original authors. 
Users must acknowledge the original sources when using these codes.
We collect and provide access to these codes solely to facilitate research and advance scientific understanding. This repository does not use or distribute these codes for any commercial purposes.
